# Arvada Replication Package

The full experiments can be re-run via
```
$ python3 run.py
```
To evaluate the recall metric via
```
$ python3 run_recall.py
```
This stores all the results for Arvada in `$HOME/arvada-results`.

*Note*: running the full experiments takes a while, because it needs to be run ten times on each benchmark

## Benchmark Structure

All the source code of benchmarks is saved in MyTask.jar.

For each benchmark `bench`, there exists a directory `$HOME/benchmarks/bench` with the following contents:
- `DSE_train_set`: the training set of examples generated by dynamic-symbolic-execution
- `GADSE_train_set`: the training set of examples generated by token-symbolic-execution
- `test_set`: the set of testing examples
- `parser.sh`: the oracle for the benchmark
